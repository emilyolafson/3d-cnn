{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env/python\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 64 # old = 128.\n",
    "    desired_height = 64\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def normalize(volume):\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n",
    "\n",
    "normal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"/home/emo4002/colossus_shared3/MSpredict/data/voxelwise_MSconnect_QSM/normal_gr3\", x)\n",
    "    for x in os.listdir(\"/home/emo4002/colossus_shared3/MSpredict/data/voxelwise_MSconnect_QSM/normal_gr3\")\n",
    "]\n",
    "\n",
    "abnormal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"/home/emo4002/colossus_shared3/MSpredict/data/voxelwise_MSconnect_QSM/disabled_gr3\", x)\n",
    "    for x in os.listdir(\"/home/emo4002/colossus_shared3/MSpredict/data/voxelwise_MSconnect_QSM/disabled_gr3\")\n",
    "]\n",
    "\n",
    "print(\"MRI scans of individuals with normal EDSS: \" + str(len(normal_scan_paths)))\n",
    "print(\"MRI scans of individuals with disabled EDSS:  \" + str(len(abnormal_scan_paths)))\n",
    "\n",
    "\n",
    "\n",
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths])\n",
    "normal_scans = np.array([process_scan(path) for path in normal_scan_paths])\n",
    "\n",
    "\n",
    "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
    "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
    "\n",
    "\n",
    "# Split data in the ratio 70-30 for training and validation.\n",
    "x_train = np.concatenate((abnormal_scans[:26], normal_scans[:97]), axis=0)\n",
    "y_train = np.concatenate((abnormal_labels[:26], normal_labels[:97]), axis=0)\n",
    "x_val = np.concatenate((abnormal_scans[18:], normal_scans[97:]), axis=0)\n",
    "y_val = np.concatenate((abnormal_labels[18:], normal_labels[97:]), axis=0)\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "\n",
    "def get_model(width=64, height=64, depth=64):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "model = get_model(width=64, height=64, depth=64)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")\n",
    "\n",
    "model_json=model.to_json()\n",
    "with open(\"/home/emo4002/colossus_shared3/MSpredict/code/results/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"/home/emo4002/colossus_shared3/MSpredict/code/results/model.h5\")\n",
    "print(\"saved model to disk\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "    ax[i].plot(model.history.history[metric])\n",
    "    ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "    ax[i].set_title(\"Model {}\".format(metric))\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].legend([\"train\", \"val\"])\n",
    "\n",
    "plt.savefig('/home/emo4002/colossus_shared3/MSpredict/code/results/acc_loss_QSM_MSConnect_2mm.png')\n",
    "\n",
    "# Load best weights.\n",
    "model.load_weights(\"3d_image_classification.h5\")\n",
    "prediction = model.predict(np.expand_dims(x_val[0], axis=0))[0]\n",
    "scores = [1 - prediction[0], prediction[0]]\n",
    "\n",
    "class_names = [\"normal\", \"abnormal\"]\n",
    "for score, name in zip(scores, class_names):\n",
    "    print(\n",
    "        \"This model is %.2f percent confident that MRI scan is %s\"\n",
    "        % ((100 * score), name)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
